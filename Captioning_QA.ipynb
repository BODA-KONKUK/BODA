{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0319 captioning and visual QA"
      ],
      "metadata": {
        "id": "8X8SB7NGiZHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a Caption"
      ],
      "metadata": {
        "id": "-Gn2lL1sifS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/KakaoTalk_Photo_2024-03-19-16-08-35.jpeg\"\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image"
      ],
      "metadata": {
        "id": "QYNLD_a8i391"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "# Salesforce/blip-image-captioning-large\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "image_to_text = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "generated_text = image_to_text(image_path)[0][\"generated_text\"]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQzKofWtiaEY",
        "outputId": "f967c09c-a932-4054-b748-80f4aeae20cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "people sitting at a table with laptops in a large room\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question and Answering about the image"
      ],
      "metadata": {
        "id": "d6uuWaaWjK0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What color is the women\\'s top? Is same with the color of man\\'s top?'\n",
        "print(\"question:\",question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59lkJQ7HjKd6",
        "outputId": "e8918d52-3899-463e-e9f2-8456fc41a1ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: What color is the women's top? Is same with the color of man's top?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMAGE QA - Salesforce/blip2-opt-2.7b"
      ],
      "metadata": {
        "id": "aq8M5KZYtWRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "from transformers import ViltProcessor\n",
        "from transformers import ViltForQuestionAnswering\n",
        "\n",
        "model_checkpoint = \"Salesforce/blip2-opt-2.7b\"\n",
        "\n",
        "processor_auto = AutoProcessor.from_pretrained(model_checkpoint)\n",
        "\n",
        "# prepare inputs\n",
        "model_blip2 = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_blip2.to(device)"
      ],
      "metadata": {
        "id": "hVM7ENaDjYKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"You are an AI assistant of blind. Please make some decription about the image based on the question. Question: {question} Answer:\"\n",
        "\n",
        "inputs_auto = processor_auto(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "generated_ids = model_blip2.generate(**inputs_auto, max_new_tokens=100, num_beams=10, #1보다 큰 값을 지정\n",
        "    early_stopping=True #EOS토큰이 나오면 생성을 중단\n",
        "                               )\n",
        "generated_text = processor_auto.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqwReBYcj3N-",
        "outputId": "1af78407-4397-42d2-be77-4f292863a318"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The woman's top is blue and the man's top is black\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMAGE QA - dandelin/vilt-b32-finetuned-vqa"
      ],
      "metadata": {
        "id": "u4YOBjTYtarz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "# prepare image + question\n",
        "image = image\n",
        "text = question\n",
        "\n",
        "processor_vilt = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model_vilt = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "\n",
        "# prepare inputs\n",
        "encoding_vilt = processor_vilt(image, text, return_tensors=\"pt\")\n",
        "\n",
        "# forward pass\n",
        "outputs_vilt = model_vilt(**encoding)\n",
        "logits = outputs_vilt.logits\n",
        "idx = logits.argmax(-1).item()\n",
        "print(\"Predicted answer:\", model_vilt.config.id2label[idx])"
      ],
      "metadata": {
        "id": "waLy9Uavj-KT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafc349c-6c5a-4f4e-d3ad-fdea20a7e18d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted answer: tan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tryDZ5tArR08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}